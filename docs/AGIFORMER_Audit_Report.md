# AGIFORMER v1.0: BirleÅŸtirilmiÅŸ Mimari Denetim & Gelecek Yol HaritasÄ±

**Tarih:** 23.11.2025
**DenetÃ§i:** Antigravity (Yapay Zeka AsistanÄ±)
**Durum:** Kod TabanÄ± ile DoÄŸrulandÄ±

## 1. YÃ¶netici Ã–zeti

KapsamlÄ± bir "Red Team" analizi ve `src/models/` dizininin doÄŸrudan kod incelemesinin ardÄ±ndan, Ã¶n raporlarda belirtilen **3 Kritik YapÄ±sal ZayÄ±flÄ±k** ve **2 KaÃ§Ä±rÄ±lmÄ±ÅŸ FÄ±rsatÄ±n** mevcut AGIFORMER v1.0 kod tabanÄ±nda **fiilen mevcut olduÄŸunu** onaylÄ±yorum.

Mevcut mimari (Faz 8), saÄŸlam bir "Kavram KanÄ±tÄ±" (Ford T) niteliÄŸindedir, ancak GPT-4 seviyesinde performans (Ferrari) iÃ§in gereken dinamik uyarlanabilirlikten yoksundur. "KÃ¶r HafÄ±za" ve "Sert Yamalama" mekanizmalarÄ±, modelin karmaÅŸÄ±k akÄ±l yÃ¼rÃ¼tme gÃ¶revlerine Ã¶lÃ§eklenmesini engelleyen en Ã¶nemli darboÄŸazlardÄ±r.

---

## 2. DoÄŸrulanmÄ±ÅŸ Zafiyetler (Kod TabanÄ± KanÄ±tlarÄ±)

### ğŸ”´ 1. Veriden BaÄŸÄ±msÄ±z Unutma ("KÃ¶r" HafÄ±za)
*   **Ciddiyet:** Kritik
*   **Konum:** `src/models/memory.py` (SatÄ±r 39, 88-89)
*   **KanÄ±t:**
    ```python
    self.decay_logits = nn.Parameter(torch.tensor([8.0] * num_heads))
    # ...
    lambdas = 0.995 + (0.005 * raw_sigmoid) # Kafa baÅŸÄ±na statik deÄŸer
    ```
*   **Etki:** Model *kÃ¼resel* bir unutma hÄ±zÄ± Ã¶ÄŸrenir. Dolgu metnini gÃ¶rmezden gelirken belirli bir ÅŸifreyi "aklÄ±nda tutmaya" karar veremez. Her ÅŸeyi aynÄ± sabit hÄ±zda unutur.

### ğŸ”´ 2. Yama SÄ±nÄ±rlarÄ±nda SÃ¼reksizlik ("Kekeleme")
*   **Ciddiyet:** YÃ¼ksek
*   **Konum:** `src/models/agiformer.py` (`LocalAutoregressiveHead`)
*   **KanÄ±t:** Dekoder, *her* yama iÃ§in `None` (sÄ±fÄ±r durum) ile baÅŸlayan bir GRU kullanÄ±r.
    ```python
    out, _ = self.rnn(rnn_input) # Gizli durum bir sonraki yamaya AKTARILMAZ
    ```
*   **Etki:** Model her 4 baytlÄ±k sÄ±nÄ±rda hafÄ±za kaybÄ± yaÅŸar. BaÄŸlamÄ± yalnÄ±zca kÃ¼resel gizli vektÃ¶rden yeniden oluÅŸturmak zorundadÄ±r, bu da metin Ã¼retiminde potansiyel "aksaklÄ±klara" veya ritim bozukluklarÄ±na yol aÃ§ar.

### ğŸ”´ 3. KÃ¶r AkÄ±l YÃ¼rÃ¼tme (Sistem 2 Ä°zolasyonu)
*   **Ciddiyet:** YÃ¼ksek
*   **Konum:** `src/models/reasoning.py`
*   **KanÄ±t:** `RecurrentReasoningBlock`, sabit `thinking_steps=3` boyunca dÃ¶ngÃ¼ye girer ve gizli vektÃ¶r `z`'yi izole bir ÅŸekilde dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
    *   DÃ¼ÅŸÃ¼nme sÃ¼reci sÄ±rasÄ±nda `HebbianMemory`'ye **eriÅŸimi yoktur**.
    *   Basit belirteÃ§ler (Ã¶rn. "ve") iÃ§in erken Ã§Ä±kÄ±ÅŸ yapamaz.
*   **Etki:** Verimsiz iÅŸlem kullanÄ±mÄ± ve akÄ±l yÃ¼rÃ¼tme aÅŸamasÄ±nda gerÃ§ekleri "arayÄ±p bulma" yeteneÄŸinin olmamasÄ±.

### ğŸŸ  4. Ä°lkel BirleÅŸtirme ("GÃ¼rÃ¼ltÃ¼lÃ¼" KarÄ±ÅŸÄ±m)
*   **Ciddiyet:** Orta
*   **Konum:** `src/models/layers.py` (`HybridBlock`)
*   **KanÄ±t:**
    ```python
    x = residual + self.out_proj(attn_out + memory_out)
    ```
*   **Etki:** Model gÃ¼rÃ¼ltÃ¼yÃ¼ kapÄ±layamaz (filtreleyemez). HafÄ±za ilgisiz Ã§aÄŸrÄ±ÅŸÄ±mlar dÃ¶ndÃ¼rÃ¼rse, bunlar zorla Dikkat Ã§Ä±ktÄ±sÄ±na eklenir. Bir kapÄ±lama mekanizmasÄ± (Ã¶rn. `sigmoid(alpha) * Mem + (1-alpha) * Attn`) eksiktir.

### ğŸŸ  5. Sert Yamalama & Seyrek GÃ¶mÃ¼lÃ¼ler
*   **Ciddiyet:** Orta
*   **Konum:** `src/models/encoder.py`
*   **KanÄ±t:**
    *   `Conv1d(stride=4, kernel_size=4)`: Ã–rtÃ¼ÅŸme yok.
    *   `nn.Embedding(256, d_model)`: Ã‡ok seyrek girdi temsili.
*   **Etki:** Yama sÄ±nÄ±rlarÄ±nda bilgi kaybÄ± ve ilk katmanlarda bayt anlambiliminin "sÄ±ÄŸ" bir ÅŸekilde anlaÅŸÄ±lmasÄ±.

---

## 3. Stratejik Yol HaritasÄ±: AGIFORMER v2.0 ("Ferrari" YÃ¼kseltmesi)

Bu bulgulara dayanarak, bir sonraki ana sÃ¼rÃ¼m iÃ§in aÅŸaÄŸÄ±daki mimari deÄŸiÅŸiklikler zorunludur.

| BileÅŸen | Ã–nerilen YÃ¼kseltme | Beklenen Fayda |
| :--- | :--- | :--- |
| **HafÄ±za** | **Girdiye BaÄŸlÄ± Unutma** (`Mamba` tarzÄ±) | $\lambda_t = \sigma(W x_t)$. Modelin Ã¶nemli bilgileri dinamik olarak hafÄ±zaya "kilitlemesini" saÄŸlar. |
| **Dekoder** | **Durumlu (Stateful) RNN / MLP** | GRU gizli durumunu yamalar arasÄ±nda taÅŸÄ±yÄ±n VEYA darboÄŸazÄ± kaldÄ±rmak iÃ§in paralel bir MLP dekoderine geÃ§in. |
| **AkÄ±l YÃ¼rÃ¼tme** | **HafÄ±za Destekli & Uyarlanabilir** | DÃ¼ÅŸÃ¼nme dÃ¶ngÃ¼sÃ¼ne HafÄ±za ile Ã‡apraz Dikkat (Cross-Attention) ekleyin. Erken Ã§Ä±kÄ±ÅŸ iÃ§in ACT (Uyarlanabilir Ä°ÅŸlem SÃ¼resi) kullanÄ±n. |
| **KodlayÄ±cÄ±** | **Ã–rtÃ¼ÅŸen Yamalar (YumuÅŸak)** | `kernel_size=6`, `stride=4` olarak deÄŸiÅŸtirin. Daha pÃ¼rÃ¼zsÃ¼z bayt entegrasyonu iÃ§in bir "kayan pencere" etkisi yaratÄ±r. |
| **BirleÅŸtirme** | **KapÄ±lÄ± (Gated) ArtÄ±klar** | Yerel Dikkat ile KÃ¼resel HafÄ±zayÄ± dengelemek iÃ§in Ã¶ÄŸrenilmiÅŸ bir kapÄ± kullanÄ±n. |
| **Ã‡ekirdek** | **SwiGLU & RMSNorm** | Daha iyi gradyan akÄ±ÅŸÄ± ve kapasite iÃ§in MLP ve Normalizasyon katmanlarÄ±nÄ± modernize edin. |

---

## 4. SonuÃ§ & Sonraki AdÄ±mlar

Mevcut **Faz 8** eÄŸitimi boÅŸa deÄŸildir. Saf bir lineer dikkat modelinin "temel zekasÄ±nÄ±" Ã¶lÃ§mek iÃ§in Ã§ok Ã¶nemli bir kÄ±yaslama noktasÄ± gÃ¶revi gÃ¶rÃ¼r.

**Acil Eylem PlanÄ±:**
1.  **Faz 8'i Tamamla:** Bir kÄ±yaslama noktasÄ± oluÅŸturmak iÃ§in mevcut eÄŸitimin bitmesine izin verin.
2.  **`test_recall.py` Ã‡alÄ±ÅŸtÄ±r:** "KÃ¶r HafÄ±za"nÄ±n *tam olarak* ne kadar kÃ¶tÃ¼ olduÄŸunu deneysel olarak Ã¶lÃ§memiz gerekiyor. Model "SamanlÄ±kta Ä°ÄŸne" testinde baÅŸarÄ±sÄ±z olursa, YÃ¼kseltme #1'in aciliyetini doÄŸrular.
3.  **v2.0 DalÄ±nÄ± HazÄ±rla:** Model eÄŸitilirken ayrÄ± bir dalda `InputDependentMemory` ve `StatefulDecoder` sÄ±nÄ±flarÄ±nÄ± kodlamaya baÅŸlayÄ±n.

**Nihai Karar:** "Red Team" haklÄ±. GÃ¼Ã§lÃ¼ bir temel inÅŸa ediyoruz, ancak "Ã§atÄ±nÄ±n" (akÄ±l yÃ¼rÃ¼tme ve uzun vadeli hatÄ±rlama) v2.0'da onarÄ±lmasÄ± gereken yapÄ±sal Ã§atlaklarÄ± var.
